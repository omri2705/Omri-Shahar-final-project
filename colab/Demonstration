# STEP 1: Install required packages
!pip install -q transformers evaluate sentence-transformers rouge-score jsonlines nltk

import random, re, difflib, torch, jsonlines
import pandas as pd
from nltk.util import ngrams
from collections import Counter
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from evaluate import load as load_metric
from sentence_transformers import SentenceTransformer, util
from google.colab import drive

# STEP 2: Mount Google Drive
drive.mount('/content/drive')

# STEP 3: Load dataset and pick a random sample
print("Loading evaluation dataset...")
dataset_path = "/content/drive/MyDrive/FinalProjectShaharAndOmriSpartans/datasets/evaluation/eval_dataset.jsonl"
with jsonlines.open(dataset_path) as reader:
    data = list(reader)

example = random.choice(data)
prompt = example["prompt"]
reference = example["response"]

# STEP 4: Load tokenizer and helper
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
device = "cuda" if torch.cuda.is_available() else "cpu"

def load_model(path):
    return GPT2LMHeadModel.from_pretrained(path).to(device).eval()

# STEP 5: Load models
base_path = "/content/drive/MyDrive/FinalProjectShaharAndOmriSpartans/FinalProjectModel/SavedModels/"

models = {
    "Base GPT2": GPT2LMHeadModel.from_pretrained("gpt2").to(device).eval(),
    "1 Epoch (100)": load_model(f"{base_path}gpt_finetuned_1_epoch/gpt2_finetuned_100"),
    "3 Epochs (100)": load_model(f"{base_path}gpt_finetuned_3_epoch/gpt2_finetuned_3_epochs_100"),
    "LoRA 3 Epochs (100)": load_model(f"{base_path}gpt_finetuned_lora_3_epoch/gpt2_lora_100"),
    "1 Epoch (5000)": load_model(f"{base_path}gpt_finetuned_1_epoch/gpt2_finetuned_5000"),
    "3 Epochs (5000)": load_model(f"{base_path}gpt_finetuned_3_epoch/gpt2_finetuned_3_epochs_5000"),
    "LoRA 3 Epochs (5000)": load_model(f"{base_path}gpt_finetuned_lora_3_epoch/gpt2_lora_5000"),
    "3 Epochs (14k)": load_model(f"{base_path}gpt_finetuned_3_epoch/gpt2_finetuned_3_epochs_14000")
}

# STEP 6: Generate predictions
def generate(model, prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    output = model.generate(**inputs, max_new_tokens=150, pad_token_id=tokenizer.eos_token_id)
    return tokenizer.decode(output[0], skip_special_tokens=True).replace(prompt, "").strip()

def extract_num_workouts(text):
    match = re.search(r"This individual is .*? so (\d+) workouts", text)
    if match:
        return int(match.group(1))
    return None

def truncate_output(output, max_workouts):
    lines = output.splitlines()
    result = []
    current_workout = 0
    for line in lines:
        if re.match(rf"^{current_workout + 1}:", line.strip()):
            current_workout += 1
        if current_workout > max_workouts:
            break
        result.append(line)
    return "\n".join(result)

outputs = {}
for name, model in models.items():
    out = generate(model, prompt)
    num = extract_num_workouts(out)
    if num:
        out = truncate_output(out, max_workouts=num)
    outputs[name] = out

# STEP 7: Evaluate
bleu = load_metric("bleu")
rouge = load_metric("rouge")
sim_model = SentenceTransformer('all-MiniLM-L6-v2')

def longest_common_subsequence(a, b):
    seq = difflib.SequenceMatcher(None, a, b)
    return ''.join(a[i1:i1+size] for i1, _, size in seq.get_matching_blocks() if size > 0)

def evaluate_metrics(output, reference):
    return {
        "BLEU": bleu.compute(predictions=[output], references=[[reference]])["bleu"],
        "ROUGE-L": rouge.compute(predictions=[output], references=[reference])["rougeL"],
        "Semantic Similarity": util.cos_sim(
            sim_model.encode(output, convert_to_tensor=True),
            sim_model.encode(reference, convert_to_tensor=True)
        ).item()
        #"LCS": longest_common_subsequence(output, reference)
    }

results = {name: evaluate_metrics(output, reference) for name, output in outputs.items()}
df = pd.DataFrame(results).T
df.index.name = "Model"

# STEP 8: Display results
print("\n📌 Prompt:\n", prompt)
print("\n✅ Reference Response:\n", reference)

print("\n📍 Model Outputs:\n")
for name, output in outputs.items():
    print(f"🔹 {name}:\n{output}\n")

print("\n📊 Evaluation Scores:")
display(df)

# STEP 9: Explanations
def explain_bleu(candidate, reference, n=4):
    explanation = ""
    for i in range(1, n+1):
        cand_ngrams = list(ngrams(candidate.split(), i))
        ref_ngrams = list(ngrams(reference.split(), i))
        overlap = len(set(cand_ngrams) & set(ref_ngrams))
        total = len(cand_ngrams)
        precision = overlap / total if total > 0 else 0
        explanation += f"{i}-gram Precision: {overlap}/{total} = {precision:.2f}\n"
    explanation += "\nBLEU = geometric mean of precisions * brevity penalty"
    return explanation

def explain_rouge_l(output, reference):
    lcs_seq = difflib.SequenceMatcher(None, output, reference).get_matching_blocks()
    lcs = ''.join(output[i.a:i.a+i.size] for i in lcs_seq if i.size > 0).strip()
    lcs_len = len(lcs.split())
    pred_len = len(output.split())
    ref_len = len(reference.split())
    recall = lcs_len / ref_len if ref_len > 0 else 0
    precision = lcs_len / pred_len if pred_len > 0 else 0
    f1 = (2 * recall * precision) / (recall + precision) if recall + precision > 0 else 0
    return (f"LCS: {' '.join(lcs.split())}\n"
            f"LCS Length: {lcs_len}, Prediction Length: {pred_len}, Reference Length: {ref_len}\n"
            f"ROUGE-L = (2 * recall * precision) / (recall + precision)\n"
            f"Recall: {recall:.2f}, Precision: {precision:.2f}, ROUGE-L : {f1:.2f}\n")
            

def explain_semantic_similarity(output, reference):
    sim = util.cos_sim(
        sim_model.encode(output, convert_to_tensor=True),
        sim_model.encode(reference, convert_to_tensor=True)
    ).item()
    return f"Cosine Similarity = dot(vec1, vec2) / (||vec1|| * ||vec2||) = {sim:.2f}"

# Print explanations
for name, output in outputs.items():
    print(f"\n🔍 Detailed Evaluation for: {name}")
    print("\n🟦 BLEU Breakdown:\n", explain_bleu(output, reference))
    print("\n🟥 ROUGE-L Breakdown:\n", explain_rouge_l(output, reference))
    print("\n🟩 Semantic Similarity Breakdown:\n", explain_semantic_similarity(output, reference))

print("""
📘 Explanation Summary:
- BLEU: Looks for shared word sequences (n-grams) between output and reference. It’s stricter and precision-based.
- ROUGE-L: Based on longest common subsequence. More recall-oriented, better for longer matches.
- Semantic Similarity: Checks whether the two texts *mean* the same thing using embeddings.
""")
